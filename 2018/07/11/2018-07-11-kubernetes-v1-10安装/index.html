<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>2018-07-11-kubernetes v1.10安装 | Leitty Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="最近打算把OneNote、有道上的一些笔记，整理下发布到博客上来。首先从搞过一段时间的DevOps工具开始。作为目前最火的项目之一，本人安排了详细的学习计划，但是平时事情太多，计划只能暂缓。 以下是在CentOs 7中通过kubeadm搭建Kubernetes v1.10的过程。前期曾通过yum install kubernetes在centos 7上直接部署了v1.5，启动都正常，但是在分发应用">
<meta name="keywords" content="DevOps,Kubernetes">
<meta property="og:type" content="article">
<meta property="og:title" content="2018-07-11-kubernetes v1.10安装">
<meta property="og:url" content="https://leitty.github.io/2018/07/11/2018-07-11-kubernetes-v1-10安装/index.html">
<meta property="og:site_name" content="Leitty Blog">
<meta property="og:description" content="最近打算把OneNote、有道上的一些笔记，整理下发布到博客上来。首先从搞过一段时间的DevOps工具开始。作为目前最火的项目之一，本人安排了详细的学习计划，但是平时事情太多，计划只能暂缓。 以下是在CentOs 7中通过kubeadm搭建Kubernetes v1.10的过程。前期曾通过yum install kubernetes在centos 7上直接部署了v1.5，启动都正常，但是在分发应用">
<meta property="og:locale" content="default">
<meta property="og:updated_time" content="2018-07-11T13:57:11.499Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="2018-07-11-kubernetes v1.10安装">
<meta name="twitter:description" content="最近打算把OneNote、有道上的一些笔记，整理下发布到博客上来。首先从搞过一段时间的DevOps工具开始。作为目前最火的项目之一，本人安排了详细的学习计划，但是平时事情太多，计划只能暂缓。 以下是在CentOs 7中通过kubeadm搭建Kubernetes v1.10的过程。前期曾通过yum install kubernetes在centos 7上直接部署了v1.5，启动都正常，但是在分发应用">
  
    <link rel="alternate" href="/atom.xml" title="Leitty Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Leitty Blog</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://leitty.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-2018-07-11-kubernetes-v1-10安装" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/07/11/2018-07-11-kubernetes-v1-10安装/" class="article-date">
  <time datetime="2018-07-11T13:21:20.000Z" itemprop="datePublished">2018-07-11</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/DevOps/">DevOps</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      2018-07-11-kubernetes v1.10安装
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>最近打算把OneNote、有道上的一些笔记，整理下发布到博客上来。首先从搞过一段时间的DevOps工具开始。作为目前最火的项目之一，本人安排了详细的学习计划，但是平时事情太多，计划只能暂缓。</p>
<p>以下是在CentOs 7中通过kubeadm搭建Kubernetes v1.10的过程。前期曾通过<code>yum install kubernetes</code>在centos 7上直接部署了v1.5，启动都正常，但是在分发应用时卡壳了，查阅资料才知道是DNS问题，1.5版本好像是SkyDNS，部署有点麻烦，可查的资料也不多，后转向了V1.10版。</p>
<h2 id="安装前的准备"><a href="#安装前的准备" class="headerlink" title="安装前的准备"></a>安装前的准备</h2><p>环境说明：三台CentOs 7的AP，IP分别是10.0.0.8，10.0.0.9，10.0.0.10</p>
<p>1.配置好各节点hosts文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">10.0.0.8  k8smaster</span><br><span class="line">10.0.0.9  k8snode1</span><br><span class="line">10.0.0.10  k8snode2</span><br></pre></td></tr></table></figure></p>
<p>2.关闭各节点系统防火墙<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl disable firewalld</span><br></pre></td></tr></table></figure></p>
<p>3.关闭各节点SElinux<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/selinux/config</span><br><span class="line"></span><br><span class="line">SELINUX=disabled</span><br></pre></td></tr></table></figure></p>
<p>4.关闭各节点swap<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">swapoff -a</span><br></pre></td></tr></table></figure></p>
<ol start="5">
<li>创建/etc/sysctl.d/k8s.conf文件<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; /etc/sysctl.d/k8s.conf</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class="line">net.bridge.bridge-nf-call-iptables = 1</span><br><span class="line">vm.swappiness=0</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">sysctl -p /etc/sysctl.d/k8s.conf</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>如果遇到报错：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sysctl: cannot stat /proc/sys/net/bridge/bridge-nf-call-ip6tables: No such file or directory</span><br><span class="line">sysctl: cannot stat /proc/sys/net/bridge/bridge-nf-call-iptables: No such file or directory</span><br></pre></td></tr></table></figure></p>
<p>需执行modprobe br_netfilter同时加入rc.local自启动中：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">modprobe br_netfilter</span><br><span class="line">echo &quot;modprobe br_netfilter&quot; &gt;&gt; /etc/rc.local</span><br></pre></td></tr></table></figure></p>
<h2 id="安装kubeadm"><a href="#安装kubeadm" class="headerlink" title="安装kubeadm"></a>安装kubeadm</h2><ol>
<li>首先配置各节点阿里K8S YUM源<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo</span><br><span class="line"> </span><br><span class="line">[kubernetes]</span><br><span class="line">name=Kubernetes</span><br><span class="line">baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=0</span><br><span class="line"> </span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">yum -y install epel-release</span><br><span class="line"> </span><br><span class="line">yum clean all</span><br><span class="line"> </span><br><span class="line">yum makecache</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>2.在各节点安装kubeadm和相关工具包<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y install docker kubelet kubeadm kubectl kubernetes-cni</span><br></pre></td></tr></table></figure></p>
<p>3.启动Docker与kubelet服务<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">systemctl enable docker &amp;&amp; systemctl start docker</span><br><span class="line"> </span><br><span class="line">systemctl enable kubelet &amp;&amp; systemctl start kubelet</span><br></pre></td></tr></table></figure></p>
<p>查看docker要与kubelet.service的cgroup要一致：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker info | grep -i cgroup</span><br><span class="line">cat /etc/systemd/system/kubelet.service.d/10-kubeadm.conf</span><br></pre></td></tr></table></figure></p>
<p>其中docker.service在以下路径：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/lib/systemd/system/docker.service</span><br></pre></td></tr></table></figure></p>
<p>改为以下方式<code>--exec-opt native.cgroupdriver=cgroupfs</code><br>执行：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -i &quot;s/cgroup-driver=systemd/cgroup-driver=cgroupfs/g&quot; /etc/systemd/system/kubelet.service.d/10-kubeadm.conf</span><br></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl daemon-reload</span><br></pre></td></tr></table></figure>
<p>4.下载K8S相关镜像（Master节点操作）<br>因为无法直接访问gcr.io下载镜像，所以需要配置一个国内的容器镜像加速器</p>
<p>配置一个阿里云的加速器：</p>
<p>登录 <a href="https://cr.console.aliyun.com/" target="_blank" rel="noopener">https://cr.console.aliyun.com/</a></p>
<p>在页面中找到并点击镜像加速按钮，即可看到属于自己的专属加速链接，选择Centos版本后即可看到配置方法。</p>
<p>提示：在阿里云上使用 Docker 并配置阿里云镜像加速器，可能会遇到 daemon.json 导致 docker daemon 无法启动的问题，可以通过以下方法解决。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/sysconfig/docker </span><br><span class="line"> </span><br><span class="line">然后 </span><br><span class="line"> </span><br><span class="line">OPTIONS=&apos;--selinux-enabled --log-driver=journald --registry-mirror=http://xxxx.mirror.aliyuncs.com&apos; </span><br><span class="line">registry-mirror 输入你的镜像地址 </span><br><span class="line"> </span><br><span class="line">最后 service docker restart 重启 daemon </span><br><span class="line"> </span><br><span class="line">然后 ps aux | grep docker 然后你就会发现带有镜像的启动参数了。</span><br></pre></td></tr></table></figure>
<p>5.下载K8S相关镜像</p>
<p>OK，解决完加速器的问题之后，开始下载k8s相关镜像，下载后将镜像名改为k8s.gcr.io/开头的名字，以便kubeadm识别使用。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line">images=(kube-proxy-amd64:v1.10.0 kube-scheduler-amd64:v1.10.0 kube-controller-manager-amd64:v1.10.0 kube-apiserver-amd64:v1.10.0</span><br><span class="line">etcd-amd64:3.1.12 pause-amd64:3.1 kubernetes-dashboard-amd64:v1.8.3 k8s-dns-sidecar-amd64:1.14.8 k8s-dns-kube-dns-amd64:1.14.8</span><br><span class="line">k8s-dns-dnsmasq-nanny-amd64:1.14.8)</span><br><span class="line">for imageName in $&#123;images[@]&#125; ; do</span><br><span class="line">  docker pull keveon/$imageName</span><br><span class="line">  docker tag keveon/$imageName k8s.gcr.io/$imageName</span><br><span class="line">  docker rmi keveon/$imageName</span><br><span class="line">done</span><br></pre></td></tr></table></figure></p>
<p>上面的shell脚本主要做了3件事，下载各种需要用到的容器镜像、重新打标记为符合k8s命令规范的版本名称、清除旧的容器镜像。</p>
<p>提示：镜像版本一定要和kubeadm安装的版本一致，否则会出现time out问题。</p>
<p>6.初始化安装K8S Master</p>
<p>执行上述shell脚本，等待下载完成后，执行kubeadm init</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubeadm init --kubernetes-version=v1.10.0 --pod-network-cidr=10.244.0.0/16</span><br></pre></td></tr></table></figure>
<p>注意记住指令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubeadm join 10.0.0.8:6443 --token a6vlug.shwfx89vqrofvro7 --discovery-token-ca-cert-hash sha256:f7b5dc65173098b1ae74b06fe5062124528ff873c53ed38761601598ddbf1a58</span><br></pre></td></tr></table></figure></p>
<p>7.配置kubectl认证信息（Master节点操作）<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># 对于非root用户</span><br><span class="line">mkdir -p $HOME/.kube</span><br><span class="line"> </span><br><span class="line">sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span><br><span class="line"> </span><br><span class="line">sudo chown $(id -u):$(id -g) $HOME/.kube/config</span><br><span class="line"> </span><br><span class="line"># 对于root用户</span><br><span class="line">export KUBECONFIG=/etc/kubernetes/admin.conf</span><br><span class="line"> </span><br><span class="line">也可以直接放到~/.bash_profile</span><br><span class="line"> </span><br><span class="line">echo &quot;export KUBECONFIG=/etc/kubernetes/admin.conf&quot; &gt;&gt; ~/.bash_profile</span><br></pre></td></tr></table></figure></p>
<p>8.安装flannel网络（Master节点操作）<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /etc/cni/net.d/</span><br></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt;EOF&gt; /etc/cni/net.d/10-flannel.conf</span><br><span class="line">&#123;</span><br><span class="line">“name”: “cbr0”,</span><br><span class="line">“type”: “flannel”,</span><br><span class="line">“delegate”: &#123;</span><br><span class="line">“isDefaultGateway”: true</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<p><code>mkdir /usr/share/oci-umount/oci-umount.d -p</code></p>
<p><code>mkdir /run/flannel/</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt;EOF&gt; /run/flannel/subnet.env</span><br><span class="line">FLANNEL_NETWORK=10.244.0.0/16</span><br><span class="line">FLANNEL_SUBNET=10.244.1.0/24</span><br><span class="line">FLANNEL_MTU=1450</span><br><span class="line">FLANNEL_IPMASQ=true</span><br><span class="line"> </span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.9.1/Documentation/kube-flannel.yml</span><br></pre></td></tr></table></figure>
<p>9.验证K8S Master是否搭建成功（Master节点操作）<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 查看pods状态</span><br><span class="line">kubectl get pods --all-namespaces</span><br></pre></td></tr></table></figure></p>
<p>注意要等到所有的pod都是running状态再加入其他节点，否则<code>kube-dns</code>、<code>kubernetes-dashboard</code>等可能会被分配到node节点上导致<code>kube-dns</code>、<code>kubernetes-dashboard</code>等安装失败</p>
<p>10.让node1、node2加入集群<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[root@k8snode2 ~]# kubeadm join 10.0.0.8:6443 --token a6vlug.shwfx89vqrofvro7 --discovery-token-ca-cert-hash sha256:f7b5dc65173098b1ae74b06fe5062124528ff873c53ed38761601598ddbf1a58</span><br><span class="line">[preflight] Running pre-flight checks.</span><br><span class="line">	[WARNING FileExisting-crictl]: crictl not found in system path</span><br><span class="line">Suggestion: go get github.com/kubernetes-incubator/cri-tools/cmd/crictl</span><br><span class="line">[preflight] Starting the kubelet service</span><br><span class="line">[discovery] Trying to connect to API Server &quot;10.0.0.8:6443&quot;</span><br><span class="line">[discovery] Created cluster-info discovery client, requesting info from &quot;https://10.0.0.8:6443&quot;</span><br><span class="line">[discovery] Requesting info from &quot;https://10.0.0.8:6443&quot; again to validate TLS against the pinned public key</span><br><span class="line">[discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server &quot;10.0.0.8:6443&quot;</span><br><span class="line">[discovery] Successfully established connection with API Server &quot;10.0.0.8:6443&quot;</span><br><span class="line"></span><br><span class="line">This node has joined the cluster:</span><br><span class="line">* Certificate signing request was sent to master and a response</span><br><span class="line">  was received.</span><br><span class="line">* The Kubelet was informed of the new secure connection details.</span><br><span class="line"></span><br><span class="line">Run &apos;kubectl get nodes&apos; on the master to see this node join the cluster.</span><br></pre></td></tr></table></figure></p>
<p>注意此段执行命令第6步中记录下来的</p>
<p>默认情况下，Master节点不参与工作负载，但如果希望安装出一个All-In-One的k8s环境，则可以执行以下命令，让Master节点也成为一个Node节点：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl taint nodes --all node-role.kubernetes.io/master-</span><br></pre></td></tr></table></figure></p>
<h2 id="安装插件"><a href="#安装插件" class="headerlink" title="安装插件"></a>安装插件</h2><p>部署dashboard（注意，dashboard需要部署在master节点上，否则会报错）</p>
<p>在k8s中 dashboard可以有两种访问方式：kubeconfig（HTTPS）和token（http）本篇先来介绍下Token方式的访问。</p>
<p>Token访问是无登录密码的，简单方便<br>1.下载官方的dashboard YAML文件或者改版的YAML（无坑版）<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 官网版</span><br><span class="line">https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml</span><br><span class="line"></span><br><span class="line"># 修改版(包括heapster插件YAML和RBAC YAML)</span><br><span class="line">https://github.com/gh-Devin/kubernetes-dashboard</span><br></pre></td></tr></table></figure></p>
<p>其中heapster.yaml的img改为<code>registry.cn-hangzhou.aliyuncs.com/google_containers/heapster:v1.5.2</code>(通过阿里云搜索到)</p>
<p>2.创建pod<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@k8smaster kubernetes-dashboard]# ls</span><br><span class="line">heapster-rbac.yaml  heapster.yaml  kubernetes-dashboard-admin.rbac.yaml  kubernetes-dashboard.yaml</span><br><span class="line">[root@k8smaster kubernetes-dashboard]# kubectl  -n kube-system create -f .</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io &quot;heapster&quot; created</span><br><span class="line">serviceaccount &quot;heapster&quot; created</span><br><span class="line">deployment.extensions &quot;heapster&quot; created</span><br><span class="line">service &quot;heapster&quot; created</span><br><span class="line">serviceaccount &quot;kubernetes-dashboard-admin&quot; created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io &quot;kubernetes-dashboard-admin&quot; created</span><br><span class="line">secret &quot;kubernetes-dashboard-certs&quot; created</span><br><span class="line">serviceaccount &quot;kubernetes-dashboard&quot; created</span><br><span class="line">role.rbac.authorization.k8s.io &quot;kubernetes-dashboard-minimal&quot; created</span><br><span class="line">rolebinding.rbac.authorization.k8s.io &quot;kubernetes-dashboard-minimal&quot; created</span><br><span class="line">deployment.apps &quot;kubernetes-dashboard&quot; created</span><br><span class="line">service &quot;kubernetes-dashboard-external&quot; created</span><br></pre></td></tr></table></figure></p>
<p>3.查看插件的状态<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get svc,pod --all-namespaces</span><br></pre></td></tr></table></figure></p>
<p>4.遇到问题查看日志<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">kubectl describe pod heapster-6595c54cb9-chmfd --namespace=kube-system </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">kubectl logs pod/heapster-6595c54cb9-chmfd -n kube-system</span><br></pre></td></tr></table></figure></p>
<p>5.安装出错，删除该pod后重新部署<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl  -n kube-system delete -f .</span><br></pre></td></tr></table></figure></p>
<h2 id="问题及解决"><a href="#问题及解决" class="headerlink" title="问题及解决"></a>问题及解决</h2><p>1、 报错：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Error from server: error dialing backend: dial tcp 192.168.0.107:10250: getsockopt: no route to host</span><br></pre></td></tr></table></figure></p>
<p>需要配置各节点上的防火墙。其中8472是flannel使用，9898和6443是minio访问master使用，centos必须配置，否则iptables -L -vn|more会看到INPUT的reject-with icmp-host-prohibited计数一直在增加。 10250是kubectl exec使用的，不加会报“Error from server: error dialing backend: dial tcp 192.168.128.164:10250: getsockopt: no route to host”。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">iptables -I INPUT -p tcp -m tcp --dport 8472 -j ACCEPT</span><br><span class="line">iptables -I INPUT -p tcp -m tcp --dport 6443 -j ACCEPT</span><br><span class="line">iptables -I INPUT -p tcp -m tcp --dport 9898 -j ACCEPT</span><br><span class="line">iptables -I INPUT -p tcp -m tcp --dport 10250 -j ACCEPT</span><br></pre></td></tr></table></figure>
<p>2、 报错：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">[controlplane] Wrote Static Pod manifest for component kube-apiserver to &quot;/etc/kubernetes/manifests/kube-apiserver.yaml&quot;</span><br><span class="line">[controlplane] Wrote Static Pod manifest for component kube-controller-manager to &quot;/etc/kubernetes/manifests/kube-controller-manager.yaml&quot;</span><br><span class="line">[controlplane] Wrote Static Pod manifest for component kube-scheduler to &quot;/etc/kubernetes/manifests/kube-scheduler.yaml&quot;</span><br><span class="line">[etcd] Wrote Static Pod manifest for a local etcd instance to &quot;/etc/kubernetes/manifests/etcd.yaml&quot;</span><br><span class="line">[init] Waiting for the kubelet to boot up the control plane as Static Pods from directory &quot;/etc/kubernetes/manifests&quot;.</span><br><span class="line">[init] This might take a minute or longer if the control plane images have to be pulled.</span><br><span class="line">[kubelet-check] It seems like the kubelet isn&apos;t running or healthy.</span><br><span class="line">[kubelet-check] The HTTP call equal to &apos;curl -sSL http://localhost:10255/healthz&apos; failed with error: Get http://localhost:10255/healthz: dial tcp [::1]:10255: getsockopt: connection refused.</span><br><span class="line">[kubelet-check] It seems like the kubelet isn&apos;t running or healthy.</span><br><span class="line">[kubelet-check] The HTTP call equal to &apos;curl -sSL http://localhost:10255/healthz&apos; failed with error: Get http://localhost:10255/healthz: dial tcp [::1]:10255: getsockopt: connection refused.</span><br><span class="line">[kubelet-check] It seems like the kubelet isn&apos;t running or healthy.</span><br><span class="line">[kubelet-check] The HTTP call equal to &apos;curl -sSL http://localhost:10255/healthz&apos; failed with error: Get http://localhost:10255/healthz: dial tcp [::1]:10255: getsockopt: connection refused.</span><br><span class="line">[kubelet-check] It seems like the kubelet isn&apos;t running or healthy.</span><br><span class="line">[kubelet-check] The HTTP call equal to &apos;curl -sSL http://localhost:10255/healthz/syncloop&apos; failed with error: Get http://localhost:10255/healthz/syncloop: dial tcp [::1]:10255: getsockopt: connection refused.</span><br><span class="line">[kubelet-check] It seems like the kubelet isn&apos;t running or healthy.</span><br><span class="line">[kubelet-check] The HTTP call equal to &apos;curl -sSL http://localhost:10255/healthz/syncloop&apos; failed with error: Get http://localhost:10255/healthz/syncloop: dial tcp [::1]:10255: getsockopt: connection refused.</span><br><span class="line">[kubelet-check] It seems like the kubelet isn&apos;t running or healthy.</span><br><span class="line">[kubelet-check] The HTTP call equal to &apos;curl -sSL http://localhost:10255/healthz/syncloop&apos; failed with error: Get http://localhost:10255/healthz/syncloop: dial tcp [::1]:10255: getsockopt: connection refused.</span><br><span class="line">[kubelet-check] It seems like the kubelet isn&apos;t running or healthy.</span><br><span class="line">[kubelet-check] The HTTP call equal to &apos;curl -sSL http://localhost:10255/healthz&apos; failed with error: Get http://localhost:10255/healthz: dial tcp [::1]:10255: getsockopt: connection refused.</span><br><span class="line">[kubelet-check] It seems like the kubelet isn&apos;t running or healthy.</span><br><span class="line">[kubelet-check] The HTTP call equal to &apos;curl -sSL http://localhost:10255/healthz/syncloop&apos; failed with error: Get http://localhost:10255/healthz/syncloop: dial tcp [::1]:10255: getsockopt: connection refused.</span><br><span class="line">[kubelet-check] It seems like the kubelet isn&apos;t running or healthy.</span><br><span class="line">[kubelet-check] The HTTP call equal to &apos;curl -sSL http://localhost:10255/healthz&apos; failed with error: Get http://localhost:10255/healthz: dial tcp [::1]:10255: getsockopt: connection refused.</span><br><span class="line"></span><br><span class="line">Unfortunately, an error has occurred:</span><br><span class="line">	timed out waiting for the condition</span><br><span class="line"></span><br><span class="line">This error is likely caused by:</span><br><span class="line">	- The kubelet is not running</span><br><span class="line">	- The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)</span><br><span class="line">	- Either there is no internet connection, or imagePullPolicy is set to &quot;Never&quot;,</span><br><span class="line">	  so the kubelet cannot pull or find the following control plane images:</span><br><span class="line">		- k8s.gcr.io/kube-apiserver-amd64:v1.10.0</span><br><span class="line">		- k8s.gcr.io/kube-controller-manager-amd64:v1.10.0</span><br><span class="line">		- k8s.gcr.io/kube-scheduler-amd64:v1.10.0</span><br><span class="line">		- k8s.gcr.io/etcd-amd64:3.1.12 (only if no external etcd endpoints are configured)</span><br><span class="line"></span><br><span class="line">If you are on a systemd-powered system, you can try to troubleshoot the error with the following commands:</span><br><span class="line">	- &apos;systemctl status kubelet&apos;</span><br><span class="line">	- &apos;journalctl -xeu kubelet&apos;</span><br></pre></td></tr></table></figure></p>
<p>docker.service 和10-kubeadm.conf中的cgroup不一致导致的</p>
<p>3、 安装dashboard报错:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@k8smaster kubernetes-dashboard]# kubectl get svc,pod --all-namespaces | grep dashboard</span><br><span class="line"></span><br><span class="line">kube-system   service/kubernetes-dashboard-external   NodePort    10.106.103.199   &lt;none&gt;        9090:30090/TCP   52s</span><br><span class="line">kube-system   pod/kubernetes-dashboard-5cc6564db9-tp5ws   0/1       CrashLoopBackOff   2          52s</span><br></pre></td></tr></table></figure></p>
<p>查看日志：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@k8smaster kubernetes-dashboard]# kubectl logs pod/kubernetes-dashboard-5cc6564db9-tp5ws -n kube-system </span><br><span class="line">2018/04/30 18:23:18 Using in-cluster config to connect to apiserver</span><br><span class="line">2018/04/30 18:23:18 Using service account token for csrf signing</span><br><span class="line">2018/04/30 18:23:18 No request provided. Skipping authorization</span><br><span class="line">2018/04/30 18:23:18 Starting overwatch</span><br><span class="line">2018/04/30 18:23:19 Error while initializing connection to Kubernetes apiserver. This most likely means that the cluster is misconfigured (e.g., it has invalid apiserver certificates or service accounts configuration) or the --apiserver-host param points to a server that does not exist. Reason: Get https://10.96.0.1:443/version: dial tcp 10.96.0.1:443: getsockopt: no route to host</span><br><span class="line">Refer to our FAQ and wiki pages for more information: https://github.com/kubernetes/dashboard/wiki/FAQ</span><br></pre></td></tr></table></figure></p>
<p>或者<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl describe pod kubernetes-dashboard-5cc6564db9-bjmgr --namespace=kube-system</span><br></pre></td></tr></table></figure></p>
<p>删除该pod：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl delete pod kubernetes-dashboard-5cc6564db9-tp5ws -n kube-system</span><br></pre></td></tr></table></figure></p>
<p>安装过程中遇到问题，可以使用一下命令清理环境，重新安装<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubeadm reset</span><br></pre></td></tr></table></figure></p>
<p>4、 安装完成后pod ip无法访问</p>
<p>极有可能是防火墙问题，需要对防火墙进行下清理</p>
<p>解决方法：</p>
<p>1.先把服务停掉<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8smaster ~]#  systemctl stop docker kubelet</span><br></pre></td></tr></table></figure></p>
<p>2.查看当前规则，然后清空<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@k8smaster ~]#  iptables -L -n</span><br><span class="line"></span><br><span class="line">[root@k8smaster ~]#  iptables -F &amp;&amp;  iptables -X &amp;&amp;  iptables -F -t nat &amp;&amp;  iptables -X -t nat</span><br><span class="line"></span><br><span class="line">[root@k8smaster ~]#  iptables -L -n</span><br><span class="line"></span><br><span class="line">Chain INPUT (policy ACCEPT)</span><br><span class="line">target     prot opt source               destination         </span><br><span class="line"></span><br><span class="line">Chain FORWARD (policy ACCEPT)</span><br><span class="line">target     prot opt source               destination         </span><br><span class="line"></span><br><span class="line">Chain OUTPUT (policy ACCEPT)</span><br><span class="line">target     prot opt source               destination      </span><br><span class="line"></span><br><span class="line">[root@hadoop38 ~]#</span><br></pre></td></tr></table></figure></p>
<p>3.重启iptables 和 开启服务<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@k8smaster ~]#  systemctl restart iptables</span><br><span class="line"></span><br><span class="line">[root@k8smaster ~]#  systemctl start docker kubelet</span><br></pre></td></tr></table></figure></p>
<p>4.再次查看防火墙策略 和清空掉<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@k8smaster ~]#  iptables -L -n</span><br><span class="line"></span><br><span class="line">[root@k8smaster ~]#  iptables -F &amp;&amp;  iptables -X &amp;&amp;  iptables -F -t nat &amp;&amp;  iptables -X -t nat</span><br></pre></td></tr></table></figure></p>
<p>5.等待一会 最终查看防火墙策略<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">[root@k8smaster ~]#  iptables -L -n</span><br><span class="line"></span><br><span class="line">Chain INPUT (policy ACCEPT)</span><br><span class="line">target     prot opt source               destination         </span><br><span class="line"></span><br><span class="line">KUBE-FIREWALL  all  --  0.0.0.0/0            0.0.0.0/0           </span><br><span class="line"></span><br><span class="line">Chain FORWARD (policy ACCEPT)</span><br><span class="line">target     prot opt source               destination         </span><br><span class="line"></span><br><span class="line">Chain OUTPUT (policy ACCEPT)</span><br><span class="line"></span><br><span class="line">target     prot opt source               destination         </span><br><span class="line"></span><br><span class="line">KUBE-FIREWALL  all  --  0.0.0.0/0            0.0.0.0/0    </span><br><span class="line"></span><br><span class="line">KUBE-SERVICES  all  --  0.0.0.0/0            0.0.0.0/0            /* kubernetes service portals */</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Chain KUBE-FIREWALL (2 references)</span><br><span class="line"></span><br><span class="line">target     prot opt source               destination         </span><br><span class="line"></span><br><span class="line">DROP       all  --  0.0.0.0/0            0.0.0.0/0            /* kubernetes firewall for dropping marked packets */ mark match 0x8000/0x8000</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Chain KUBE-SERVICES (1 references)</span><br><span class="line">target     prot opt source               destination         </span><br><span class="line"></span><br><span class="line">[root@k8smaster ~]#          destination</span><br></pre></td></tr></table></figure></p>
<p>6.验证<br>部署nginx：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl run nginx --image=nginx --replicas=2 --port=80</span><br></pre></td></tr></table></figure></p>
<p>暴露端口：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl expose deployment/nginx --type=&quot;NodePort&quot; --port 80</span><br></pre></td></tr></table></figure></p>
<p>查看应用状态：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">[root@k8smaster ~]# kubectl describe svc</span><br><span class="line">Name:              kubernetes</span><br><span class="line">Namespace:         default</span><br><span class="line">Labels:            component=apiserver</span><br><span class="line">                   provider=kubernetes</span><br><span class="line">Annotations:       &lt;none&gt;</span><br><span class="line">Selector:          &lt;none&gt;</span><br><span class="line">Type:              ClusterIP</span><br><span class="line">IP:                10.96.0.1</span><br><span class="line">Port:              https  443/TCP</span><br><span class="line">TargetPort:        6443/TCP</span><br><span class="line">Endpoints:         10.0.0.8:6443</span><br><span class="line">Session Affinity:  ClientIP</span><br><span class="line">Events:            &lt;none&gt;</span><br><span class="line"></span><br><span class="line">Name:                     nginx</span><br><span class="line">Namespace:                default</span><br><span class="line">Labels:                   run=nginx</span><br><span class="line">Annotations:              &lt;none&gt;</span><br><span class="line">Selector:                 run=nginx</span><br><span class="line">Type:                     NodePort</span><br><span class="line">IP:                       10.98.84.61</span><br><span class="line">Port:                     &lt;unset&gt;  80/TCP</span><br><span class="line">TargetPort:               80/TCP</span><br><span class="line">NodePort:                 &lt;unset&gt;  30019/TCP</span><br><span class="line">Endpoints:                10.244.1.60:80,10.244.1.61:80</span><br><span class="line">Session Affinity:         None</span><br><span class="line">External Traffic Policy:  Cluster</span><br><span class="line">Events:                   &lt;none&gt;</span><br></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[root@k8smaster ~]# ping 10.244.1.60</span><br><span class="line">PING 10.244.1.60 (10.244.1.60) 56(84) bytes of data.</span><br><span class="line">64 bytes from 10.244.1.60: icmp_seq=1 ttl=63 time=0.941 ms</span><br><span class="line">64 bytes from 10.244.1.60: icmp_seq=2 ttl=63 time=1.01 ms</span><br><span class="line">^C</span><br><span class="line">--- 10.244.1.60 ping statistics ---</span><br><span class="line">2 packets transmitted, 2 received, 0% packet loss, time 1003ms</span><br><span class="line">rtt min/avg/max/mdev = 0.941/0.979/1.017/0.038 ms</span><br><span class="line">[root@k8smaster ~]# ping 10.244.1.61</span><br><span class="line">PING 10.244.1.61 (10.244.1.61) 56(84) bytes of data.</span><br><span class="line">64 bytes from 10.244.1.61: icmp_seq=1 ttl=63 time=0.781 ms</span><br><span class="line">64 bytes from 10.244.1.61: icmp_seq=2 ttl=63 time=0.574 ms</span><br><span class="line">^C</span><br><span class="line">--- 10.244.1.61 ping statistics ---</span><br><span class="line">2 packets transmitted, 2 received, 0% packet loss, time 1000ms</span><br><span class="line">rtt min/avg/max/mdev = 0.574/0.677/0.781/0.106 ms</span><br><span class="line">[root@k8smaster ~]# ping 10.244.1.61</span><br><span class="line">PING 10.244.1.61 (10.244.1.61) 56(84) bytes of data.</span><br><span class="line">64 bytes from 10.244.1.61: icmp_seq=1 ttl=63 time=0.630 ms</span><br><span class="line">64 bytes from 10.244.1.61: icmp_seq=2 ttl=63 time=0.662 ms</span><br><span class="line">^C</span><br></pre></td></tr></table></figure>
<p>5、 报错：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Failed to get system container stats ``for &quot;/system.slice/</span><br></pre></td></tr></table></figure></p>
<p>解决：</p>
<p>在/etc/systemd/system/kubelet.service中加入：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ExecStart=/usr/bin/kubelet --runtime-cgroups=/systemd/system.slice --kubelet-cgroups=/systemd/system.slice</span><br></pre></td></tr></table></figure></p>
<p>参考：</p>
<p><a href="https://kubernetes.io/docs" target="_blank" rel="noopener">k8s官方文档</a></p>
<p><a href="https://github.com/kubernetes/dashboard#kubernetes-dashboard" target="_blank" rel="noopener">kubernetes-dashboard Github地址</a></p>
<p><a href="http://blog.51cto.com/devingeng/2096495?from=singlemessage" target="_blank" rel="noopener">51CTO 使用kubeadm安装Kubernetes v1.10以及常见问题解答
</a></p>
<p><a href="http://blog.51cto.com/devingeng/2096495?from=singlemessage" target="_blank" rel="noopener">51CTO Kubernetes1.10中部署dashboard以及常见问题解析
</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://leitty.github.io/2018/07/11/2018-07-11-kubernetes-v1-10安装/" data-id="cjjlc5vml0004s4vl5kdp6ky4" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/DevOps/">DevOps</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2018/07/12/2018-07-12-jenkins简介/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          2018-07-12-jenkins简介
        
      </div>
    </a>
  
  
    <a href="/2018/07/08/2018-07-08-搭建自己的github-io博客/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">搭建自己的github.io博客</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Blog/">Blog</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/DevOps/">DevOps</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Blog/">Blog</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DevOps/">DevOps</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Github-io/">Github.io</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hexo/">Hexo</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Jenkins/">Jenkins</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Blog/" style="font-size: 10px;">Blog</a> <a href="/tags/DevOps/" style="font-size: 20px;">DevOps</a> <a href="/tags/Github-io/" style="font-size: 10px;">Github.io</a> <a href="/tags/Hexo/" style="font-size: 10px;">Hexo</a> <a href="/tags/Jenkins/" style="font-size: 10px;">Jenkins</a> <a href="/tags/Kubernetes/" style="font-size: 10px;">Kubernetes</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/07/">July 2018</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2018/07/12/2018-07-12-jenkins简介/">2018-07-12-jenkins简介</a>
          </li>
        
          <li>
            <a href="/2018/07/11/2018-07-11-kubernetes-v1-10安装/">2018-07-11-kubernetes v1.10安装</a>
          </li>
        
          <li>
            <a href="/2018/07/08/2018-07-08-搭建自己的github-io博客/">搭建自己的github.io博客</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2018 Leitty<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>